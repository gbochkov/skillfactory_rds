{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация изображений (Car Classification)\n",
    "## Unit 7 Project 5\n",
    "\n",
    "### Основная идея решения: взять предобученую сеть и дообучить под задачу проекта; на основе рекомендаций улучшить качество модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновление pip\n",
    "!pip install --upgrade pip\n",
    "# Обновление tensorflow\n",
    "!pip install tensorflow --upgrade\n",
    "# Загрузка модели efficientnet\n",
    "!pip install -q efficientnet\n",
    "# Загружаем обвязку под keras для использования продвинутых библиотек аугментации, например, albuminations\n",
    "!pip install git+https://github.com/mjkvaak/ImageDataAugmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Загрузка необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import zipfile\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as C\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter\n",
    "\n",
    "# увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "# графики в svg выглядят более четкими\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "print('Keras        :', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем настройки\n",
    "EPOCHS = 20  # эпох на обучение 10\n",
    "BATCH_SIZE = 8  # уменьшаем batch до 8 для работы с моделью EfficientNetB6\n",
    "LR = 1e-4\n",
    "VAL_SPLIT = 0.20  # сколько данных выделяем на тест = 20%\n",
    "\n",
    "CLASS_NUM = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE = 320  # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS = 3   # у RGB 3 канала\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "# пути актуальны для notebook в Kaggle\n",
    "DATA_PATH = '../input/sf-dl-car-classification/'\n",
    "PATH = \"../working/car/\"  # рабочая директория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устаналиваем конкретное значение random seed для воспроизводимости\n",
    "# os.makedirs(PATH,exist_ok=False)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Category.value_counts()\n",
    "# распределение классов достаточно равномерное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распаковывам исходные данные\n",
    "print('Распаковываем картинки')\n",
    "for data_zip in ['train.zip', 'test.zip']:\n",
    "    with zipfile.ZipFile(DATA_PATH+data_zip, \"r\") as z:\n",
    "        z.extractall(PATH)\n",
    "\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Пример картинок (random sample)')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "random_image = train_df.sample(n=9)\n",
    "random_image_paths = random_image['Id'].values\n",
    "random_image_cat = random_image['Category'].values\n",
    "\n",
    "for index, path in enumerate(random_image_paths):\n",
    "    im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n",
    "    plt.subplot(3, 3, index+1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Class: '+str(random_image_cat[index]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# образец изображения для анализа\n",
    "# на основе данных образца определяем параметры изображений для дальнейшей обработки\n",
    "image = PIL.Image.open(PATH+'/train/0/100380.jpg')\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментация данных\n",
    "В отличии от baseline используем более продвинутую библиотеку аугментации изображений\n",
    "Параметны аугументации подбирались опытным путем в процессе работы над проектом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# после первых опытов был уменьшен лимит ротации и увеличены значеия кропа\n",
    "AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=10, interpolation=1, border_mode=4,\n",
    "                          value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.CenterCrop(height=240, width=320),\n",
    "        albumentations.CenterCrop(height=320, width=240),\n",
    "    ], p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.3, contrast_limit=0.3),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1, contrast_limit=0.1)\n",
    "    ], p=0.5),\n",
    "    albumentations.GaussianBlur(p=0.05),\n",
    "    albumentations.HueSaturationValue(p=0.5),\n",
    "    albumentations.RGBShift(p=0.5),\n",
    "    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "    rescale=1./255,\n",
    "    augment=AUGMENTATIONS,\n",
    "    validation_split=VAL_SPLIT,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataAugmentor(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # set as validation data\n",
    "\n",
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример изображений из генератора\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "def imshow(image_RGB):\n",
    "    io.imshow(image_RGB)\n",
    "    io.show()\n",
    "\n",
    "\n",
    "x, y = train_generator.next()\n",
    "print('Пример картинок из train_generator')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    image = x[i]\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(image)\n",
    "    #plt.title('Class: '+str(y[i]))\n",
    "    # plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "За основу берем сеть EfficientNetB6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим модель на базе предобученной сети EfficientNetB6\n",
    "base_model = efn.EfficientNetB6(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заморозим веса imagenet в базовой модели, чтобы она работала в качестве feature extractor \n",
    "# и наша голова обучалась делать классификацию на наши 10 классов\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для повышения производительности и стабилизации работы добавляем в \"новую голову\" batch нормализацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем новую \"голову\" (head):\n",
    "model = M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D(),)\n",
    "model.add(L.Dense(512, activation='relu'))\n",
    "model.add(L.BatchNormalization())  # добавляем Batch-нормализацию\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best_model.hdf5', monitor=[\n",
    "                             'val_accuracy'], verbose=1, mode='max')\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                          patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step1: Обучаем \"голову\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n",
    "model.save('../working/model_step1.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(\n",
    "    test_generator, steps=len(test_generator), verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'g', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # plt.figure()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : FineTuning, разморозка половины слоев базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers)//2\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../working/model_step2.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : размораживаем 75% базовой модели  \n",
    "(на финише от этого этапа я отказался для оптизации времени исполнения notebook'а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "# fine_tune_at = len(base_model.layers)//4\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#   layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(base_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR = 1e-5\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "# history = model.fit_generator(\n",
    "#        train_generator,\n",
    "#        steps_per_epoch = train_generator.samples//train_generator.batch_size,\n",
    "#        validation_data = test_generator,\n",
    "#        validation_steps = test_generator.samples//test_generator.batch_size,\n",
    "#        epochs = 10,\n",
    "#        callbacks = callbacks_list\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../working/model_step3.hdf5')\n",
    "# model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: размораживаем базовую сеть полностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../working/model_step4.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.reset()\n",
    "predictions = model.predict(\n",
    "    test_sub_generator, steps=len(test_sub_generator), verbose=1)\n",
    "predictions = np.argmax(predictions, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TTA\n",
    "Аугментируем тестовые изображения и сделаем несколько предсказаний одной картинки в разном виде. Взяв среднее значение из нескольких предсказаний получим итоговое предсказание(по факту результат с этой методикой получил хуже, поэтому в Kaggle были загружены результаты предыдущего этапа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=[0.75, 1.25],\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_steps = 10\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    preds = model.predict(test_sub_generator, steps=len(\n",
    "        test_sub_generator), verbose=1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(pred, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission_tta.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PATH\n",
    "import shutil\n",
    "shutil.rmtree(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Был применен transfer learning с fine-tuning.\n",
    "2. Настройте LR.\n",
    "3. При подготовке данных были подобраны: размер картинки, размер батча.\n",
    "4. При обучение модели: была использована \"другая сеть\" EfficientNetB6, добавлена Batch Normalization, проведены эксперименты с архитектурой «головы», применена дополнительная функции callback в Keras, количество эпох увеличено до 20.\n",
    "5. Добавлена TTA (Test Time Augmentation).\n",
    "6. Добавлена более продвинутая библиотека аугментации изображений (albumentations). \n",
    "7. В соревновании Kaggle достигнут результат: 0.97393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
